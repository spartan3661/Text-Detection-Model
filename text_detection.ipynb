{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBackbone(nn.Module): # get C2, C3, C4, and C5\n",
    "    def __init__(self, backbone='resnet18', pretrained=True):\n",
    "        super(ResnetBackbone, self).__init__()\n",
    "\n",
    "        # feature extractor\n",
    "        self.backbone = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        print(self.backbone)\n",
    "        self.feature_channels=[64, 128, 256, 512] # dimension for c1, c2, c3, c4 in resnet 18\n",
    "\n",
    "        # remove last two layers of resnet18\n",
    "        model_layers = list(self.backbone.children())[:-2]\n",
    "        self.backbone = nn.Sequential(*model_layers)\n",
    "        #print(self.backbone) # view resnet18 model\n",
    "\n",
    "        self.stem = nn.Sequential(                  #contains first residual block of resnet\n",
    "            self.backbone[0],                       # conv1\n",
    "            self.backbone[1],                       # batch norm\n",
    "            self.backbone[2],                       # ReLU\n",
    "            self.backbone[3]                        # max pooling\n",
    "        )\n",
    "\n",
    "        self.layer1 = self.backbone[4]\n",
    "        self.layer2 = self.backbone[5]\n",
    "        self.layer3 = self.backbone[6]\n",
    "        self.layer4 = self.backbone[7]\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.stem(x)\n",
    "        c2 = self.layer1(c1)\n",
    "        c3 = self.layer2(c2)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "        return c2, c3, c4, c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ResnetBackbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, backbone, out_channel=256): # out_channel represents fixed number of features outputs for C5 - C2\n",
    "        super(FPN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # lateral convolution, merges C# -> M#\n",
    "        self.lateral_layers = nn.ModuleList([\n",
    "            nn.Conv2d(self.backbone.feature_channels[3], out_channel, kernel_size=1), # feature channel corresponds to layers in ResnetBackbone, eg. feature_channel[3] = C5\n",
    "            nn.Conv2d(self.backbone.feature_channels[2], out_channel, kernel_size=1),\n",
    "            nn.Conv2d(self.backbone.feature_channels[1], out_channel, kernel_size=1),\n",
    "            nn.Conv2d(self.backbone.feature_channels[0], out_channel, kernel_size=1)   \n",
    "        ])\n",
    "\n",
    "        # smoothing 3x3 filters, smooths M4, M3, M2 -> P4, P3, P2\n",
    "        self.smoothing_layers = nn.ModuleList([         #module list stores sub modules like Sequential but layer sequence can be changed in forward pass\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        c2, c3, c4, c5 = self.backbone(x) #bottom up path from resnet backbone\n",
    "\n",
    "        m5 = self.lateral_layers[0](c5) #C5 -> M5, skip upsampling\n",
    "        m4 = self.lateral_layers[1](c4) + F.interpolate(m5, size=c4.shape[2:], mode='nearest') #include smoothing and upsampling\n",
    "        m3 = self.lateral_layers[2](c3) + F.interpolate(m4, size=c3.shape[2:], mode='nearest')\n",
    "        m2 = self.lateral_layers[3](c2) + F.interpolate(m3, size=c2.shape[2:], mode='nearest')\n",
    "\n",
    "\n",
    "        p5 = m5\n",
    "        p4 = self.smoothing_layers[0](m4)\n",
    "        p3 = self.smoothing_layers[1](m3)\n",
    "        p2 = self.smoothing_layers[2](m2)\n",
    "\n",
    "        return [p2, p3, p4, p5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels=256, type_classes=3, legibility_classes=2):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        #shared convolution layer\n",
    "\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # classification and regression heads\n",
    "\n",
    "        self.type_head = nn.Conv2d(256, type_classes, kernel_size=1)\n",
    "        self.bbox_head = nn.Conv2d(256, 4, kernel_size=1)\n",
    "        self.legibility_head = nn.Conv2d(256, legibility_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        type_preds = []\n",
    "        bbox_preds = []\n",
    "        legibility_preds = []\n",
    "\n",
    "        for feature_map in feature_maps:\n",
    "            x = self.shared_conv(feature_map)\n",
    "            type_preds.append(self.type_head(x))\n",
    "            bbox_preds.append(self.bbox_head(x))\n",
    "            legibility_preds.append(self.legibility_head(x))\n",
    "\n",
    "        predictions = {\n",
    "            \"type\": type_preds,\n",
    "            \"bbox\": bbox_preds,\n",
    "            \"legibility\": legibility_preds\n",
    "        }\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDetectionModel(nn.Module):\n",
    "    def __init__(self, type_classes=2, legibility_classes=2):\n",
    "        super(TextDetectionModel, self).__init__(),\n",
    "\n",
    "        self.backbone = ResnetBackbone()\n",
    "        self.FPN = FPN(self.backbone)\n",
    "        self.detection_head = DetectionHead(\n",
    "            in_channels=256,\n",
    "            type_classes=type_classes,\n",
    "            legibility_classes=legibility_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fpn_features = self.FPN(x)\n",
    "\n",
    "        predictions = self.detection_head(fpn_features)\n",
    "\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, annotation_file, image_dir, transform=None):\n",
    "        with open(annotation_file, 'r') as ann_file:\n",
    "            self.data = json.load(ann_file)\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_ids = list(self.data['imgs'].keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_metadata = self.data['imgs'][img_id]\n",
    "\n",
    "        annotations = [\n",
    "            ann for ann in self.data['anns'].values()\n",
    "            if ann['image_id'] == int(img_id)\n",
    "        ]\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_metadata['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        width, height = image.size\n",
    "        \n",
    "        scale_w = 256 / width\n",
    "        scale_h = 256 / height\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        bboxes = []\n",
    "        type_labels = []\n",
    "        legibility_labels = []\n",
    "\n",
    "        for ann in annotations:\n",
    "            bbox = ann.get('bbox', [0, 0, 0, 0])\n",
    "            category = ann.get('class', \"unknown\")  # default to \"unknown\"\n",
    "            legibility = ann.get('legibility', \"illegible\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(bbox) == 4:\n",
    "                x1, y1, w, h = bbox\n",
    "                scaled_bbox = [\n",
    "                    x1 * scale_w,\n",
    "                    y1 * scale_h,\n",
    "                    (x1 + w) * scale_w,\n",
    "                    (y1 + h) * scale_h\n",
    "                ]\n",
    "                bboxes.append(scaled_bbox)\n",
    "            else:\n",
    "                bboxes.append([0, 0, 0, 0])  # placeholder\n",
    "\n",
    "            if category == \"machine printed\":\n",
    "                type_labels.append(1)  # 1 represents \"machine printed\"\n",
    "            elif category == \"handwritten\":\n",
    "                type_labels.append(2)  # 2 represents \"handwritten\"\n",
    "            elif category == \"others\":\n",
    "                type_labels.append(3)\n",
    "            else:\n",
    "                type_labels.append(0)  # 0 for unknown\n",
    "\n",
    "            if legibility == \"legible\":\n",
    "                legibility_labels.append(1)  # 1 for legible\n",
    "            else:\n",
    "                legibility_labels.append(0)  # 2 for illegible\n",
    "\n",
    "        return image, {\n",
    "            'bboxes': torch.tensor(bboxes, dtype=torch.float32),\n",
    "            'type_labels': torch.tensor(type_labels, dtype=torch.int64),\n",
    "            'legibility_labels': torch.tensor(legibility_labels, dtype=torch.int64),\n",
    "            'image_id': img_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    type_labels = []\n",
    "    legibility_labels = []\n",
    "    image_ids = []\n",
    "\n",
    "    for image, annotation in batch:\n",
    "        images.append(image)\n",
    "        \n",
    "        if len(annotation['bboxes']) == 0:\n",
    "            annotation['bboxes'] = torch.tensor([[0, 0, 0, 0]], dtype=torch.float32)\n",
    "            annotation['type_labels'] = torch.tensor([0], dtype=torch.int64)\n",
    "            annotation['legibility_labels'] = torch.tensor([0], dtype=torch.int64)\n",
    "        \n",
    "        bboxes.append(annotation['bboxes'])\n",
    "        type_labels.append(annotation['type_labels'])\n",
    "        legibility_labels.append(annotation['legibility_labels'])\n",
    "        image_ids.append(annotation['image_id'])\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    bboxes_padded = pad_sequence(bboxes, batch_first=True, padding_value=-1)\n",
    "    type_labels_padded = pad_sequence(type_labels, batch_first=True, padding_value=-1)\n",
    "    legibility_labels_padded = pad_sequence(legibility_labels, batch_first=True, padding_value=-1)\n",
    "\n",
    "    return {\n",
    "        'images': images,\n",
    "        'bboxes': bboxes_padded,\n",
    "        'type_labels': type_labels_padded,\n",
    "        'legibility_labels': legibility_labels_padded,\n",
    "        'image_ids': image_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "annotation_file = \"./COCO-Text/COCO_Text.json\"\n",
    "image_dir = \"./COCO-Text/train2014\"\n",
    "\n",
    "dataset = COCODataset(annotation_file=annotation_file, image_dir=image_dir, transform=transform)\n",
    "dataset_size = len(dataset)\n",
    "train_size = 0.8 * dataset_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42)) #ensure same split everytime\n",
    "print(dataset_size)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "for i in range(20):\n",
    "    image, annotations = train_dataset[i]\n",
    "    \n",
    "    print(f\"Example {i + 1}\")\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Bounding Boxes:\", annotations['bboxes'])\n",
    "    print(\"Type Labels:\", annotations['type_labels'])\n",
    "    print(\"Legibility Labels:\", annotations['legibility_labels'])\n",
    "    print(\"Image ID:\", annotations['image_id'])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "index = random.randrange(8)\n",
    "for batch_idx, batch in enumerate(val_dataloader):\n",
    "    print(index)\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(\"Images Shape:\", batch['images'].shape)\n",
    "    print(\"Bboxes Shape:\", batch['bboxes'].shape)\n",
    "    print(\"Type Labels Shape:\", batch['type_labels'].shape)\n",
    "    print(\"Legibility Labels Shape:\", batch['legibility_labels'].shape)\n",
    "    print(\"Image IDs:\", batch['image_ids'])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Details of the first entry in the batch:\")\n",
    "    print(\"Image shape:\", batch['images'][index].shape)\n",
    "    print(\"Bounding Boxes:\", batch['bboxes'][index])\n",
    "    print(\"Type Labels:\", batch['type_labels'][index])\n",
    "    print(\"Legibility Labels:\", batch['legibility_labels'][index])\n",
    "    print(\"Image ID:\", batch['image_ids'][index])\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter >=1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_free_assign_targets(predictions, bboxes, type_labels, legibility_labels, image_sizes, strides=[4, 8, 16, 32]):\n",
    "    # To Be Implemented\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextDetectionModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "bbox_loss_fn = nn.MSELoss()  # placeholder loss functions\n",
    "type_loss_fn = nn.CrossEntropyLoss() \n",
    "legibility_loss_fn = nn.CrossEntropyLoss()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
